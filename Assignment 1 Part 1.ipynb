{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Assignment 1/Part - 2A"}, {"cell_type": "code", "metadata": {}, "source": "#Configration 1\n#Average Pooling with relu\n\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n\nimport os\nimport pickle\nimport numpy as np\n\nbatch_size = 32\nnum_classes = 10\nepochs = 22\ndata_augmentation = True\nnum_predictions = 20\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'keras_cifar10_trained_model_1P.h5'\n\n# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel_1P = Sequential()\n\nmodel_1P.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_1P.add(Activation('relu'))\nmodel_1P.add(Conv2D(32, (3, 3)))\nmodel_1P.add(Activation('relu'))\nmodel_1P.add(AveragePooling2D(pool_size=(2, 2)))\nmodel_1P.add(Dropout(0.25))\n\nmodel_1P.add(Conv2D(64, (3, 3), padding='same'))\nmodel_1P.add(Activation('relu'))\nmodel_1P.add(Conv2D(64, (3, 3)))\nmodel_1P.add(Activation('relu'))\nmodel_1P.add(AveragePooling2D(pool_size=(2, 2)))\nmodel_1P.add(Dropout(0.25))\n\nmodel_1P.add(Flatten())\nmodel_1P.add(Dense(512))\nmodel_1P.add(Activation('relu'))\nmodel_1P.add(Dropout(0.5))\nmodel_1P.add(Dense(num_classes))\nmodel_1P.add(Activation('softmax'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_1P.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model_1P.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model_1P.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test),\n                        workers=4)\n\n# Load label names to use in prediction results\nlabel_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n\n\nkeras_dir = os.path.expanduser(os.path.join('~', '.keras'))\ndatadir_base = os.path.expanduser(keras_dir)\nif not os.access(datadir_base, os.W_OK):\n    datadir_base = os.path.join('/tmp', '.keras')\nlabel_list_path = os.path.join(datadir_base, label_list_path)\n\nwith open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)\n\n# Evaluate model with test data set and share sample prediction results\nevaluation = model_1P.evaluate_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))\n\npredict_gen = model_1P.predict_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\n\nfor predict_index, predicted_y in enumerate(predict_gen):\n    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break", "outputs": [{"text": "x_train shape: (50000, 32, 32, 3)\n50000 train samples\n10000 test samples\nUsing real-time data augmentation.\nEpoch 1/22\n1562/1562 [==============================] - 106s - loss: 1.8580 - acc: 0.3181 - val_loss: 1.6289 - val_acc: 0.4145\nEpoch 2/22\n1562/1562 [==============================] - 100s - loss: 1.6055 - acc: 0.4127 - val_loss: 1.3994 - val_acc: 0.5008\nEpoch 3/22\n1562/1562 [==============================] - 104s - loss: 1.4928 - acc: 0.4570 - val_loss: 1.3591 - val_acc: 0.5154\nEpoch 4/22\n1562/1562 [==============================] - 102s - loss: 1.4194 - acc: 0.4898 - val_loss: 1.2625 - val_acc: 0.5482\nEpoch 5/22\n1562/1562 [==============================] - 107s - loss: 1.3551 - acc: 0.5156 - val_loss: 1.1952 - val_acc: 0.5729\nEpoch 6/22\n1562/1562 [==============================] - 102s - loss: 1.3036 - acc: 0.5348 - val_loss: 1.1264 - val_acc: 0.5995\nEpoch 7/22\n1562/1562 [==============================] - 101s - loss: 1.2571 - acc: 0.5523 - val_loss: 1.1303 - val_acc: 0.6031\nEpoch 8/22\n1562/1562 [==============================] - 105s - loss: 1.2108 - acc: 0.5690 - val_loss: 1.0849 - val_acc: 0.6162\nEpoch 9/22\n1562/1562 [==============================] - 104s - loss: 1.1775 - acc: 0.5819 - val_loss: 1.0352 - val_acc: 0.6325\nEpoch 10/22\n1562/1562 [==============================] - 102s - loss: 1.1486 - acc: 0.5940 - val_loss: 1.0252 - val_acc: 0.6417\nEpoch 11/22\n1562/1562 [==============================] - 95s - loss: 1.1115 - acc: 0.6088 - val_loss: 0.9856 - val_acc: 0.6483\nEpoch 12/22\n1562/1562 [==============================] - 95s - loss: 1.0895 - acc: 0.6157 - val_loss: 0.9770 - val_acc: 0.6545\nEpoch 13/22\n1562/1562 [==============================] - 100s - loss: 1.0696 - acc: 0.6211 - val_loss: 0.9380 - val_acc: 0.6664\nEpoch 14/22\n1562/1562 [==============================] - 100s - loss: 1.0487 - acc: 0.6298 - val_loss: 0.9268 - val_acc: 0.6766\nEpoch 15/22\n1562/1562 [==============================] - 100s - loss: 1.0316 - acc: 0.6351 - val_loss: 0.8911 - val_acc: 0.6859\nEpoch 16/22\n1562/1562 [==============================] - 102s - loss: 1.0166 - acc: 0.6410 - val_loss: 0.8719 - val_acc: 0.6898\nEpoch 17/22\n1562/1562 [==============================] - 103s - loss: 1.0014 - acc: 0.6465 - val_loss: 0.9065 - val_acc: 0.6860\nEpoch 18/22\n1562/1562 [==============================] - 106s - loss: 0.9859 - acc: 0.6514 - val_loss: 0.8468 - val_acc: 0.7040\nEpoch 19/22\n1562/1562 [==============================] - 105s - loss: 0.9735 - acc: 0.6566 - val_loss: 0.8644 - val_acc: 0.6944\nEpoch 20/22\n1562/1562 [==============================] - 107s - loss: 0.9521 - acc: 0.6653 - val_loss: 0.8709 - val_acc: 0.6949\nEpoch 21/22\n1562/1562 [==============================] - 109s - loss: 0.9459 - acc: 0.6686 - val_loss: 0.8047 - val_acc: 0.7182\nEpoch 22/22\n1562/1562 [==============================] - 105s - loss: 0.9325 - acc: 0.6732 - val_loss: 0.7964 - val_acc: 0.7236\nModel Accuracy = 0.70\nActual Label = cat vs. Predicted Label = cat\nActual Label = ship vs. Predicted Label = ship\nActual Label = ship vs. Predicted Label = ship\nActual Label = airplane vs. Predicted Label = airplane\nActual Label = frog vs. Predicted Label = frog\nActual Label = frog vs. Predicted Label = frog\nActual Label = automobile vs. Predicted Label = automobile\nActual Label = frog vs. Predicted Label = frog\nActual Label = cat vs. Predicted Label = cat\nActual Label = automobile vs. Predicted Label = automobile\nActual Label = airplane vs. Predicted Label = deer\nActual Label = truck vs. Predicted Label = truck\nActual Label = dog vs. Predicted Label = dog\nActual Label = horse vs. Predicted Label = horse\nActual Label = truck vs. Predicted Label = truck\nActual Label = ship vs. Predicted Label = bird\nActual Label = dog vs. Predicted Label = dog\nActual Label = horse vs. Predicted Label = horse\nActual Label = ship vs. Predicted Label = ship\nActual Label = frog vs. Predicted Label = frog\nActual Label = horse vs. Predicted Label = horse\n", "name": "stdout", "output_type": "stream"}], "execution_count": 11}, {"cell_type": "code", "metadata": {}, "source": "#Configration 2\n#Default Setting: Relu with Max Pooling\n\n\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n\nimport os\nimport pickle\nimport numpy as np\n\nbatch_size = 32\nnum_classes = 10\nepochs = 22\ndata_augmentation = True\nnum_predictions = 20\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'keras_cifar10_trained_model_1A.h5'\n\n# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel_1A = Sequential()\n\nmodel_1A.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(Conv2D(32, (3, 3)))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1A.add(Dropout(0.25))\n\nmodel_1A.add(Conv2D(64, (3, 3), padding='same'))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(Conv2D(64, (3, 3)))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1A.add(Dropout(0.25))\n\nmodel_1A.add(Flatten())\nmodel_1A.add(Dense(512))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(Dropout(0.5))\nmodel_1A.add(Dense(num_classes))\nmodel_1A.add(Activation('softmax'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_1A.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model_1A.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model_1A.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test),\n                        workers=4)\n\n# Load label names to use in prediction results\nlabel_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n\n\nkeras_dir = os.path.expanduser(os.path.join('~', '.keras'))\ndatadir_base = os.path.expanduser(keras_dir)\nif not os.access(datadir_base, os.W_OK):\n    datadir_base = os.path.join('/tmp', '.keras')\nlabel_list_path = os.path.join(datadir_base, label_list_path)\n\nwith open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)\n\n# Evaluate model with test data set and share sample prediction results\nevaluation = model_1A.evaluate_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))\n\npredict_gen = model_1A.predict_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\n\nfor predict_index, predicted_y in enumerate(predict_gen):\n    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break", "outputs": [{"text": "x_train shape: (50000, 32, 32, 3)\n50000 train samples\n10000 test samples\nUsing real-time data augmentation.\nEpoch 1/22\n1562/1562 [==============================] - 105s - loss: 1.7554 - acc: 0.3684 - val_loss: 1.5188 - val_acc: 0.4671\nEpoch 2/22\n1562/1562 [==============================] - 102s - loss: 1.5200 - acc: 0.4593 - val_loss: 1.3490 - val_acc: 0.5165\nEpoch 3/22\n1562/1562 [==============================] - 104s - loss: 1.3835 - acc: 0.5095 - val_loss: 1.2315 - val_acc: 0.5723\nEpoch 4/22\n1562/1562 [==============================] - 103s - loss: 1.3082 - acc: 0.5418 - val_loss: 1.2024 - val_acc: 0.5759\nEpoch 5/22\n1562/1562 [==============================] - 102s - loss: 1.2615 - acc: 0.5604 - val_loss: 1.1082 - val_acc: 0.6134\nEpoch 6/22\n1562/1562 [==============================] - 102s - loss: 1.2280 - acc: 0.5736 - val_loss: 1.0907 - val_acc: 0.6229\nEpoch 7/22\n1562/1562 [==============================] - 101s - loss: 1.1995 - acc: 0.5851 - val_loss: 1.0872 - val_acc: 0.6169\nEpoch 8/22\n1562/1562 [==============================] - 101s - loss: 1.1781 - acc: 0.5924 - val_loss: 1.0221 - val_acc: 0.6486\nEpoch 9/22\n1562/1562 [==============================] - 102s - loss: 1.1541 - acc: 0.6016 - val_loss: 1.0397 - val_acc: 0.6423\nEpoch 10/22\n1562/1562 [==============================] - 103s - loss: 1.1440 - acc: 0.6034 - val_loss: 1.0081 - val_acc: 0.6539\nEpoch 11/22\n1562/1562 [==============================] - 103s - loss: 1.1311 - acc: 0.6095 - val_loss: 1.0180 - val_acc: 0.6461\nEpoch 12/22\n1562/1562 [==============================] - 103s - loss: 1.1115 - acc: 0.6148 - val_loss: 1.0059 - val_acc: 0.6504\nEpoch 13/22\n1562/1562 [==============================] - 94s - loss: 1.0973 - acc: 0.6219 - val_loss: 0.9890 - val_acc: 0.6524\nEpoch 14/22\n1562/1562 [==============================] - 96s - loss: 1.0886 - acc: 0.6255 - val_loss: 0.9399 - val_acc: 0.6772\nEpoch 15/22\n1562/1562 [==============================] - 96s - loss: 1.0778 - acc: 0.6271 - val_loss: 0.9346 - val_acc: 0.6801\nEpoch 16/22\n1562/1562 [==============================] - 94s - loss: 1.0719 - acc: 0.6278 - val_loss: 0.9298 - val_acc: 0.6787\nEpoch 17/22\n1562/1562 [==============================] - 94s - loss: 1.0594 - acc: 0.6328 - val_loss: 0.9310 - val_acc: 0.6799\nEpoch 18/22\n1562/1562 [==============================] - 95s - loss: 1.0530 - acc: 0.6356 - val_loss: 0.9447 - val_acc: 0.6734\nEpoch 19/22\n1562/1562 [==============================] - 94s - loss: 1.0467 - acc: 0.6376 - val_loss: 0.9159 - val_acc: 0.6784\nEpoch 20/22\n1562/1562 [==============================] - 94s - loss: 1.0343 - acc: 0.6414 - val_loss: 0.8888 - val_acc: 0.6921\nEpoch 21/22\n1562/1562 [==============================] - 95s - loss: 1.0222 - acc: 0.6475 - val_loss: 0.9014 - val_acc: 0.6911\nEpoch 22/22\n1562/1562 [==============================] - 94s - loss: 1.0159 - acc: 0.6477 - val_loss: 0.8786 - val_acc: 0.6976\nModel Accuracy = 0.67\nActual Label = cat vs. Predicted Label = frog\nActual Label = ship vs. Predicted Label = airplane\nActual Label = ship vs. Predicted Label = airplane\nActual Label = airplane vs. Predicted Label = horse\nActual Label = frog vs. Predicted Label = deer\nActual Label = frog vs. Predicted Label = dog\nActual Label = automobile vs. Predicted Label = frog\nActual Label = frog vs. Predicted Label = frog\nActual Label = cat vs. Predicted Label = automobile\nActual Label = automobile vs. Predicted Label = automobile\nActual Label = airplane vs. Predicted Label = frog\nActual Label = truck vs. Predicted Label = frog\nActual Label = dog vs. Predicted Label = ship\nActual Label = horse vs. Predicted Label = horse\nActual Label = truck vs. Predicted Label = deer\nActual Label = ship vs. Predicted Label = automobile\nActual Label = dog vs. Predicted Label = bird\nActual Label = horse vs. Predicted Label = bird\nActual Label = ship vs. Predicted Label = automobile\nActual Label = frog vs. Predicted Label = frog\nActual Label = horse vs. Predicted Label = airplane\n", "name": "stdout", "output_type": "stream"}], "execution_count": 12}, {"cell_type": "code", "metadata": {}, "source": "#Configration 3\n\n#Using Tanh Activation Function \n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n\nimport os\nimport pickle\nimport numpy as np\n\nbatch_size = 32\nnum_classes = 10\nepochs = 22\ndata_augmentation = True\nnum_predictions = 20\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'keras_cifar10_trained_model_1A.h5'\n\n# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel_1A = Sequential()\n\nmodel_1A.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(Conv2D(32, (3, 3)))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1A.add(Dropout(0.25))\n\nmodel_1A.add(Conv2D(64, (3, 3), padding='same'))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(Conv2D(64, (3, 3)))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1A.add(Dropout(0.25))\n\nmodel_1A.add(Flatten())\nmodel_1A.add(Dense(512))\nmodel_1A.add(Activation('tanh'))\nmodel_1A.add(Dropout(0.5))\nmodel_1A.add(Dense(num_classes))\nmodel_1A.add(Activation('softmax'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_1A.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model_1A.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model_1A.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test),\n                        workers=4)\n\n# Load label names to use in prediction results\nlabel_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n\n\nkeras_dir = os.path.expanduser(os.path.join('~', '.keras'))\ndatadir_base = os.path.expanduser(keras_dir)\nif not os.access(datadir_base, os.W_OK):\n    datadir_base = os.path.join('/tmp', '.keras')\nlabel_list_path = os.path.join(datadir_base, label_list_path)\n\nwith open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)\n\n# Evaluate model with test data set and share sample prediction results\nevaluation = model_1A.evaluate_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))\n\npredict_gen = model_1A.predict_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\n\nfor predict_index, predicted_y in enumerate(predict_gen):\n    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break", "outputs": [{"text": "x_train shape: (50000, 32, 32, 3)\n50000 train samples\n10000 test samples\nUsing real-time data augmentation.\nEpoch 1/22\n1562/1562 [==============================] - 97s - loss: 1.7408 - acc: 0.3745 - val_loss: 1.5062 - val_acc: 0.4562\nEpoch 2/22\n1562/1562 [==============================] - 94s - loss: 1.4935 - acc: 0.4713 - val_loss: 1.2814 - val_acc: 0.5509\nEpoch 3/22\n1562/1562 [==============================] - 94s - loss: 1.3761 - acc: 0.5140 - val_loss: 1.2519 - val_acc: 0.5611\nEpoch 4/22\n1562/1562 [==============================] - 94s - loss: 1.2949 - acc: 0.5468 - val_loss: 1.1607 - val_acc: 0.5887\nEpoch 5/22\n1562/1562 [==============================] - 94s - loss: 1.2519 - acc: 0.5640 - val_loss: 1.1096 - val_acc: 0.6190\nEpoch 6/22\n1562/1562 [==============================] - 94s - loss: 1.2214 - acc: 0.5755 - val_loss: 1.0762 - val_acc: 0.6265\nEpoch 7/22\n1562/1562 [==============================] - 93s - loss: 1.1931 - acc: 0.5873 - val_loss: 1.0733 - val_acc: 0.6279\nEpoch 8/22\n1562/1562 [==============================] - 94s - loss: 1.1784 - acc: 0.5915 - val_loss: 1.0184 - val_acc: 0.6544\nEpoch 9/22\n1562/1562 [==============================] - 94s - loss: 1.1544 - acc: 0.6003 - val_loss: 1.0191 - val_acc: 0.6496\nEpoch 10/22\n1562/1562 [==============================] - 93s - loss: 1.1422 - acc: 0.6057 - val_loss: 1.0184 - val_acc: 0.6492\nEpoch 11/22\n1562/1562 [==============================] - 94s - loss: 1.1258 - acc: 0.6110 - val_loss: 1.0412 - val_acc: 0.6437\nEpoch 12/22\n1562/1562 [==============================] - 95s - loss: 1.1125 - acc: 0.6166 - val_loss: 0.9829 - val_acc: 0.6611\nEpoch 13/22\n1562/1562 [==============================] - 95s - loss: 1.1020 - acc: 0.6199 - val_loss: 0.9634 - val_acc: 0.6721\nEpoch 14/22\n1562/1562 [==============================] - 97s - loss: 1.0933 - acc: 0.6234 - val_loss: 0.9609 - val_acc: 0.6682\nEpoch 15/22\n1562/1562 [==============================] - 101s - loss: 1.0820 - acc: 0.6268 - val_loss: 0.9380 - val_acc: 0.6752\nEpoch 16/22\n1562/1562 [==============================] - 97s - loss: 1.0694 - acc: 0.6315 - val_loss: 0.9297 - val_acc: 0.6786\nEpoch 17/22\n1562/1562 [==============================] - 97s - loss: 1.0616 - acc: 0.6324 - val_loss: 0.9293 - val_acc: 0.6765\nEpoch 18/22\n1562/1562 [==============================] - 99s - loss: 1.0535 - acc: 0.6354 - val_loss: 0.9311 - val_acc: 0.6793\nEpoch 19/22\n1562/1562 [==============================] - 99s - loss: 1.0452 - acc: 0.6379 - val_loss: 0.9239 - val_acc: 0.6829\nEpoch 20/22\n1562/1562 [==============================] - 96s - loss: 1.0383 - acc: 0.6427 - val_loss: 0.9332 - val_acc: 0.6749\nEpoch 21/22\n1562/1562 [==============================] - 98s - loss: 1.0308 - acc: 0.6446 - val_loss: 0.8855 - val_acc: 0.6945\nEpoch 22/22\n1562/1562 [==============================] - 94s - loss: 1.0203 - acc: 0.6472 - val_loss: 0.8768 - val_acc: 0.6943\nModel Accuracy = 0.67\nActual Label = cat vs. Predicted Label = frog\nActual Label = ship vs. Predicted Label = ship\nActual Label = ship vs. Predicted Label = ship\nActual Label = airplane vs. Predicted Label = ship\nActual Label = frog vs. Predicted Label = frog\nActual Label = frog vs. Predicted Label = frog\nActual Label = automobile vs. Predicted Label = automobile\nActual Label = frog vs. Predicted Label = frog\nActual Label = cat vs. Predicted Label = cat\nActual Label = automobile vs. Predicted Label = automobile\nActual Label = airplane vs. Predicted Label = airplane\nActual Label = truck vs. Predicted Label = truck\nActual Label = dog vs. Predicted Label = dog\nActual Label = horse vs. Predicted Label = horse\nActual Label = truck vs. Predicted Label = truck\nActual Label = ship vs. Predicted Label = ship\nActual Label = dog vs. Predicted Label = dog\nActual Label = horse vs. Predicted Label = horse\nActual Label = ship vs. Predicted Label = ship\nActual Label = frog vs. Predicted Label = frog\nActual Label = horse vs. Predicted Label = horse\n", "name": "stdout", "output_type": "stream"}], "execution_count": 13}, {"cell_type": "code", "metadata": {}, "source": "#Configration 4\n#Using elu with maxpooling\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n\nimport os\nimport pickle\nimport numpy as np\n\nbatch_size = 32\nnum_classes = 10\nepochs = 22\ndata_augmentation = True\nnum_predictions = 20\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'keras_cifar10_trained_model_1A.h5'\n\n# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel_1A = Sequential()\n\nmodel_1A.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_1A.add(Activation('elu'))\nmodel_1A.add(Conv2D(32, (3, 3)))\nmodel_1A.add(Activation('elu'))\nmodel_1A.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1A.add(Dropout(0.25))\n\nmodel_1A.add(Conv2D(64, (3, 3), padding='same'))\nmodel_1A.add(Activation('elu'))\nmodel_1A.add(Conv2D(64, (3, 3)))\nmodel_1A.add(Activation('elu'))\nmodel_1A.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1A.add(Dropout(0.25))\n\nmodel_1A.add(Flatten())\nmodel_1A.add(Dense(512))\nmodel_1A.add(Activation('elu'))\nmodel_1A.add(Dropout(0.5))\nmodel_1A.add(Dense(num_classes))\nmodel_1A.add(Activation('softmax'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_1A.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model_1A.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model_1A.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test),\n                        workers=4)\n\n# Load label names to use in prediction results\nlabel_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n\n\nkeras_dir = os.path.expanduser(os.path.join('~', '.keras'))\ndatadir_base = os.path.expanduser(keras_dir)\nif not os.access(datadir_base, os.W_OK):\n    datadir_base = os.path.join('/tmp', '.keras')\nlabel_list_path = os.path.join(datadir_base, label_list_path)\n\nwith open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)\n\n# Evaluate model with test data set and share sample prediction results\nevaluation = model_1A.evaluate_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))\n\npredict_gen = model_1A.predict_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\n\nfor predict_index, predicted_y in enumerate(predict_gen):\n    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break", "outputs": [{"text": "x_train shape: (50000, 32, 32, 3)\n50000 train samples\n10000 test samples\nUsing real-time data augmentation.\nEpoch 1/22\n1562/1562 [==============================] - 98s - loss: 1.7388 - acc: 0.3746 - val_loss: 1.4903 - val_acc: 0.4627\nEpoch 2/22\n1562/1562 [==============================] - 102s - loss: 1.4786 - acc: 0.4754 - val_loss: 1.2765 - val_acc: 0.5576\nEpoch 3/22\n1562/1562 [==============================] - 101s - loss: 1.3625 - acc: 0.5193 - val_loss: 1.2545 - val_acc: 0.5510\nEpoch 4/22\n1562/1562 [==============================] - 102s - loss: 1.2869 - acc: 0.5499 - val_loss: 1.1229 - val_acc: 0.6101\nEpoch 5/22\n1562/1562 [==============================] - 101s - loss: 1.2376 - acc: 0.5693 - val_loss: 1.1660 - val_acc: 0.5925\nEpoch 6/22\n1562/1562 [==============================] - 101s - loss: 1.2061 - acc: 0.5806 - val_loss: 1.0943 - val_acc: 0.6097\nEpoch 7/22\n1562/1562 [==============================] - 98s - loss: 1.1742 - acc: 0.5919 - val_loss: 1.0531 - val_acc: 0.6320\nEpoch 8/22\n1562/1562 [==============================] - 95s - loss: 1.1576 - acc: 0.5987 - val_loss: 1.0122 - val_acc: 0.6473\nEpoch 9/22\n1562/1562 [==============================] - 96s - loss: 1.1369 - acc: 0.6063 - val_loss: 0.9931 - val_acc: 0.6542\nEpoch 10/22\n1562/1562 [==============================] - 97s - loss: 1.1143 - acc: 0.6149 - val_loss: 0.9948 - val_acc: 0.6530\nEpoch 11/22\n1562/1562 [==============================] - 96s - loss: 1.0937 - acc: 0.6207 - val_loss: 0.9511 - val_acc: 0.6751\nEpoch 12/22\n1562/1562 [==============================] - 97s - loss: 1.0824 - acc: 0.6265 - val_loss: 0.9435 - val_acc: 0.6720\nEpoch 13/22\n1562/1562 [==============================] - 100s - loss: 1.0671 - acc: 0.6290 - val_loss: 0.9098 - val_acc: 0.6887\nEpoch 14/22\n1562/1562 [==============================] - 102s - loss: 1.0508 - acc: 0.6343 - val_loss: 0.8901 - val_acc: 0.6916\nEpoch 15/22\n1562/1562 [==============================] - 99s - loss: 1.0425 - acc: 0.6386 - val_loss: 0.8991 - val_acc: 0.6855\nEpoch 16/22\n1562/1562 [==============================] - 103s - loss: 1.0222 - acc: 0.6462 - val_loss: 0.8636 - val_acc: 0.7008\nEpoch 17/22\n1562/1562 [==============================] - 100s - loss: 1.0145 - acc: 0.6485 - val_loss: 0.8500 - val_acc: 0.7071\nEpoch 18/22\n1562/1562 [==============================] - 99s - loss: 0.9982 - acc: 0.6533 - val_loss: 0.8520 - val_acc: 0.7034\nEpoch 19/22\n1562/1562 [==============================] - 99s - loss: 0.9883 - acc: 0.6577 - val_loss: 0.8342 - val_acc: 0.7120\nEpoch 20/22\n1562/1562 [==============================] - 101s - loss: 0.9689 - acc: 0.6656 - val_loss: 0.8106 - val_acc: 0.7203\nEpoch 21/22\n1562/1562 [==============================] - 100s - loss: 0.9624 - acc: 0.6652 - val_loss: 0.8106 - val_acc: 0.7174\nEpoch 22/22\n1562/1562 [==============================] - 102s - loss: 0.9518 - acc: 0.6712 - val_loss: 0.7771 - val_acc: 0.7293\nModel Accuracy = 0.71\nActual Label = cat vs. Predicted Label = frog\nActual Label = ship vs. Predicted Label = cat\nActual Label = ship vs. Predicted Label = truck\nActual Label = airplane vs. Predicted Label = automobile\nActual Label = frog vs. Predicted Label = deer\nActual Label = frog vs. Predicted Label = automobile\nActual Label = automobile vs. Predicted Label = truck\nActual Label = frog vs. Predicted Label = dog\nActual Label = cat vs. Predicted Label = airplane\nActual Label = automobile vs. Predicted Label = frog\nActual Label = airplane vs. Predicted Label = dog\nActual Label = truck vs. Predicted Label = frog\nActual Label = dog vs. Predicted Label = airplane\nActual Label = horse vs. Predicted Label = truck\nActual Label = truck vs. Predicted Label = cat\nActual Label = ship vs. Predicted Label = truck\nActual Label = dog vs. Predicted Label = deer\nActual Label = horse vs. Predicted Label = frog\nActual Label = ship vs. Predicted Label = truck\nActual Label = frog vs. Predicted Label = ship\nActual Label = horse vs. Predicted Label = frog\n", "name": "stdout", "output_type": "stream"}], "execution_count": 14}, {"cell_type": "code", "metadata": {}, "source": "#Configration 5\n#Maxpooling with leakyrelu\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, advanced_activations\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n\nimport os\nimport pickle\nimport numpy as np\n\nbatch_size = 32\nnum_classes = 10\nepochs = 22\ndata_augmentation = True\nnum_predictions = 20\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'keras_cifar10_trained_model_1A.h5'\n\n# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel_1A = Sequential()\n\nmodel_1A.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_1A.add(advanced_activations.LeakyReLU(alpha=0.3))\nmodel_1A.add(Conv2D(32, (3, 3)))\nmodel_1A.add(advanced_activations.LeakyReLU(alpha=0.3))\nmodel_1A.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1A.add(Dropout(0.25))\n\nmodel_1A.add(Conv2D(64, (3, 3), padding='same'))\nmodel_1A.add(advanced_activations.LeakyReLU(alpha=0.3))\nmodel_1A.add(Conv2D(64, (3, 3)))\nmodel_1A.add(advanced_activations.LeakyReLU(alpha=0.3))\nmodel_1A.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1A.add(Dropout(0.25))\n\nmodel_1A.add(Flatten())\nmodel_1A.add(Dense(512))\nmodel_1A.add(advanced_activations.LeakyReLU(alpha=0.3))\nmodel_1A.add(Dropout(0.5))\nmodel_1A.add(Dense(num_classes))\nmodel_1A.add(Activation('softmax'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_1A.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model_1A.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model_1A.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test),\n                        workers=4)\n\n# Load label names to use in prediction results\nlabel_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n\n\nkeras_dir = os.path.expanduser(os.path.join('~', '.keras'))\ndatadir_base = os.path.expanduser(keras_dir)\nif not os.access(datadir_base, os.W_OK):\n    datadir_base = os.path.join('/tmp', '.keras')\nlabel_list_path = os.path.join(datadir_base, label_list_path)\n\nwith open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)\n\n# Evaluate model with test data set and share sample prediction results\nevaluation = model_1A.evaluate_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))\n\npredict_gen = model_1A.predict_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\n\nfor predict_index, predicted_y in enumerate(predict_gen):\n    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break", "outputs": [{"text": "x_train shape: (50000, 32, 32, 3)\n50000 train samples\n10000 test samples\nUsing real-time data augmentation.\nEpoch 1/22\n1562/1562 [==============================] - 122s - loss: 1.8069 - acc: 0.3419 - val_loss: 1.5099 - val_acc: 0.4600\nEpoch 2/22\n1562/1562 [==============================] - 124s - loss: 1.5274 - acc: 0.4461 - val_loss: 1.3228 - val_acc: 0.5392\nEpoch 3/22\n1562/1562 [==============================] - 123s - loss: 1.4048 - acc: 0.4972 - val_loss: 1.2489 - val_acc: 0.5633\nEpoch 4/22\n1562/1562 [==============================] - 124s - loss: 1.3119 - acc: 0.5359 - val_loss: 1.1365 - val_acc: 0.5974\nEpoch 5/22\n1562/1562 [==============================] - 123s - loss: 1.2387 - acc: 0.5615 - val_loss: 1.0562 - val_acc: 0.6316\nEpoch 6/22\n1562/1562 [==============================] - 128s - loss: 1.1713 - acc: 0.5889 - val_loss: 0.9992 - val_acc: 0.6496\nEpoch 7/22\n1562/1562 [==============================] - 120s - loss: 1.1168 - acc: 0.6069 - val_loss: 0.9461 - val_acc: 0.6719\nEpoch 8/22\n1562/1562 [==============================] - 120s - loss: 1.0766 - acc: 0.6234 - val_loss: 0.9054 - val_acc: 0.6864\nEpoch 9/22\n1562/1562 [==============================] - 123s - loss: 1.0373 - acc: 0.6379 - val_loss: 0.9050 - val_acc: 0.6930\nEpoch 10/22\n1562/1562 [==============================] - 123s - loss: 1.0063 - acc: 0.6472 - val_loss: 0.8826 - val_acc: 0.6898\nEpoch 11/22\n1562/1562 [==============================] - 124s - loss: 0.9855 - acc: 0.6576 - val_loss: 0.8378 - val_acc: 0.7149\nEpoch 12/22\n1562/1562 [==============================] - 125s - loss: 0.9615 - acc: 0.6652 - val_loss: 0.8218 - val_acc: 0.7206\nEpoch 13/22\n1562/1562 [==============================] - 123s - loss: 0.9503 - acc: 0.6707 - val_loss: 0.7930 - val_acc: 0.7284\nEpoch 14/22\n1562/1562 [==============================] - 123s - loss: 0.9242 - acc: 0.6781 - val_loss: 0.7922 - val_acc: 0.7288\nEpoch 15/22\n1562/1562 [==============================] - 123s - loss: 0.9131 - acc: 0.6848 - val_loss: 0.7784 - val_acc: 0.7342\nEpoch 16/22\n1562/1562 [==============================] - 122s - loss: 0.8953 - acc: 0.6882 - val_loss: 0.7503 - val_acc: 0.7459\nEpoch 17/22\n1562/1562 [==============================] - 123s - loss: 0.8807 - acc: 0.6927 - val_loss: 0.7568 - val_acc: 0.7404\nEpoch 18/22\n1562/1562 [==============================] - 123s - loss: 0.8730 - acc: 0.6975 - val_loss: 0.7594 - val_acc: 0.7437\nEpoch 19/22\n1562/1562 [==============================] - 124s - loss: 0.8596 - acc: 0.7014 - val_loss: 0.7457 - val_acc: 0.7438\nEpoch 20/22\n1562/1562 [==============================] - 122s - loss: 0.8498 - acc: 0.7036 - val_loss: 0.7168 - val_acc: 0.7532\nEpoch 21/22\n1562/1562 [==============================] - 122s - loss: 0.8384 - acc: 0.7075 - val_loss: 0.7222 - val_acc: 0.7525\nEpoch 22/22\n1562/1562 [==============================] - 120s - loss: 0.8325 - acc: 0.7117 - val_loss: 0.7146 - val_acc: 0.7559\nModel Accuracy = 0.74\nActual Label = cat vs. Predicted Label = deer\nActual Label = ship vs. Predicted Label = cat\nActual Label = ship vs. Predicted Label = truck\nActual Label = airplane vs. Predicted Label = bird\nActual Label = frog vs. Predicted Label = deer\nActual Label = frog vs. Predicted Label = automobile\nActual Label = automobile vs. Predicted Label = truck\nActual Label = frog vs. Predicted Label = dog\nActual Label = cat vs. Predicted Label = airplane\nActual Label = automobile vs. Predicted Label = frog\nActual Label = airplane vs. Predicted Label = dog\nActual Label = truck vs. Predicted Label = frog\nActual Label = dog vs. Predicted Label = airplane\nActual Label = horse vs. Predicted Label = truck\nActual Label = truck vs. Predicted Label = cat\nActual Label = ship vs. Predicted Label = truck\nActual Label = dog vs. Predicted Label = deer\nActual Label = horse vs. Predicted Label = deer\nActual Label = ship vs. Predicted Label = truck\nActual Label = frog vs. Predicted Label = ship\nActual Label = horse vs. Predicted Label = frog\n", "name": "stdout", "output_type": "stream"}], "execution_count": 16}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.0", "name": "python2-spark20", "language": "python"}, "language_info": {"mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}, "file_extension": ".py", "version": "2.7.11", "nbconvert_exporter": "python"}}, "nbformat_minor": 1, "nbformat": 4}