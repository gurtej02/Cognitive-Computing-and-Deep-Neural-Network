{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.stats import itemfreq\n",
    "import scipy\n",
    "from keras.metrics import categorical_accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s599-c0b5c7066f8945-dec0cd3d7a06/.local/lib/python2.7/site-packages\r\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages (from h5py)\r\n",
      "Requirement already satisfied: six in /usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages (from h5py)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defaults and constants\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIM = RMSprop(lr= LEARNING_RATE)\n",
    "LOSS_FUNC = \"categorical_crossentropy\"\n",
    "ACT_FUNC = \"relu\"\n",
    "DROPOUT = 0.25\n",
    "NEURONS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (50000, 32, 32, 3))\n",
      "(50000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to categorical (1 hot enconding)\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with 1 hidden layer\n",
    "def model_shallow(BATCH_SIZE, NB_EPOCH, NB_CLASSES, VALIDATION_SPLIT, LEARNING_RATE, OPTIM, LOSS_FUNC, ACT_FUNC, DROPOUT, MODEL_NAME,\n",
    "                 MODEL_WEIGHT, NEURONS):\n",
    "    #network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "    model.add(Activation(ACT_FUNC))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(NEURONS))\n",
    "    model.add(Activation(ACT_FUNC))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NB_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    # train\n",
    "    model.compile(loss=LOSS_FUNC, optimizer=OPTIM,\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "              epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "              verbose=VERBOSE)\n",
    "    score = model.evaluate(X_test, Y_test,\n",
    "                           batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "    print(\"Test score:\", score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    #save model\n",
    "    model_json = model.to_json()\n",
    "    open(MODEL_NAME, 'w').write(model_json)\n",
    "    \n",
    "    #And the weights learned by our deep network on the training set\n",
    "    model.save_weights(MODEL_WEIGHT, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with multiple hidden layers\n",
    "def model_deep(BATCH_SIZE, NB_EPOCH, NB_CLASSES, VALIDATION_SPLIT, LEARNING_RATE, OPTIM, LOSS_FUNC, ACT_FUNC, DROPOUT, MODEL_NAME,\n",
    "                 MODEL_WEIGHT, NEURONS):\n",
    "    #network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "    model.add(Activation(ACT_FUNC))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation(ACT_FUNC))\n",
    "    model.add(Conv2D(64, 3, 3))\n",
    "    model.add(Activation(ACT_FUNC))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(NEURONS))\n",
    "    model.add(Activation(ACT_FUNC))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NB_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # train\n",
    "    model.compile(loss=LOSS_FUNC, optimizer=OPTIM,\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "              epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "              verbose=VERBOSE)\n",
    "    score = model.evaluate(X_test, Y_test,\n",
    "                           batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "    print(\"Test score:\", score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    #save model\n",
    "    model_json = model.to_json()\n",
    "    open(MODEL_NAME, 'w').write(model_json)\n",
    "\n",
    "    #And the weights learned by our deep network on the training set\n",
    "    model.save_weights(MODEL_WEIGHT, overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paraconfig import configration_1, configration_2, configration_3, configration_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 86s - loss: 1.5901 - acc: 0.4243 - val_loss: 1.2132 - val_acc: 0.5683\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 92s - loss: 1.1531 - acc: 0.5959 - val_loss: 0.9655 - val_acc: 0.6606\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 94s - loss: 1.0108 - acc: 0.6535 - val_loss: 0.8760 - val_acc: 0.6936\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 94s - loss: 0.9496 - acc: 0.6779 - val_loss: 0.9100 - val_acc: 0.6854\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 94s - loss: 0.9281 - acc: 0.6881 - val_loss: 0.8093 - val_acc: 0.7239\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 88s - loss: 0.9310 - acc: 0.6875 - val_loss: 0.8611 - val_acc: 0.7045\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 93s - loss: 0.9400 - acc: 0.6916 - val_loss: 0.9112 - val_acc: 0.6889\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 95s - loss: 0.9650 - acc: 0.6831 - val_loss: 0.8357 - val_acc: 0.7154\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 89s - loss: 0.9949 - acc: 0.6790 - val_loss: 0.9227 - val_acc: 0.7027\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 87s - loss: 1.0277 - acc: 0.6697 - val_loss: 0.8871 - val_acc: 0.7074\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 86s - loss: 1.0493 - acc: 0.6652 - val_loss: 1.0066 - val_acc: 0.6686\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 90s - loss: 1.0867 - acc: 0.6570 - val_loss: 1.1748 - val_acc: 0.6416\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 86s - loss: 1.1201 - acc: 0.6469 - val_loss: 0.8871 - val_acc: 0.7071\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 84s - loss: 1.1555 - acc: 0.6341 - val_loss: 1.0299 - val_acc: 0.6605\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 88s - loss: 1.2083 - acc: 0.6209 - val_loss: 1.1995 - val_acc: 0.6285\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 84s - loss: 1.2425 - acc: 0.6096 - val_loss: 1.1931 - val_acc: 0.6009\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 85s - loss: 1.2929 - acc: 0.5923 - val_loss: 1.1862 - val_acc: 0.6227\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 85s - loss: 1.3271 - acc: 0.5784 - val_loss: 1.3725 - val_acc: 0.5442\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 85s - loss: 1.3509 - acc: 0.5761 - val_loss: 1.4240 - val_acc: 0.4936\n",
      "Epoch 20/20\n",
      "20736/40000 [==============>...............] - ETA: 38s - loss: 1.3742 - acc: 0.5648"
     ]
    }
   ],
   "source": [
    "#configration 1 is the default configration as defined by the base code\n",
    "#We are using model_deep as the base model had multiple hidden layers\n",
    "model_deep(*configration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 73s - loss: 1.2957 - acc: 0.5465 - val_loss: 1.1894 - val_acc: 0.5887\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 72s - loss: 1.2059 - acc: 0.5816 - val_loss: 1.1761 - val_acc: 0.5995\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 71s - loss: 1.1516 - acc: 0.6072 - val_loss: 1.1115 - val_acc: 0.6244\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 67s - loss: 1.1212 - acc: 0.6198 - val_loss: 1.1728 - val_acc: 0.5944\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 45s - loss: 1.1049 - acc: 0.6281 - val_loss: 1.0888 - val_acc: 0.6317\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 47s - loss: 1.0882 - acc: 0.6321 - val_loss: 1.0754 - val_acc: 0.6439\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 73s - loss: 1.0715 - acc: 0.6399 - val_loss: 1.1291 - val_acc: 0.6164\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 73s - loss: 1.0696 - acc: 0.6409 - val_loss: 1.1458 - val_acc: 0.6271\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 72s - loss: 1.0634 - acc: 0.6488 - val_loss: 1.1404 - val_acc: 0.6184\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 72s - loss: 1.0616 - acc: 0.6515 - val_loss: 1.1745 - val_acc: 0.6311\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.0590 - acc: 0.6525 - val_loss: 1.1663 - val_acc: 0.6172\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.0594 - acc: 0.6571 - val_loss: 1.1427 - val_acc: 0.6319\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 67s - loss: 1.0647 - acc: 0.6507 - val_loss: 1.1813 - val_acc: 0.6340\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 67s - loss: 1.0600 - acc: 0.6527 - val_loss: 1.1765 - val_acc: 0.6328\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 67s - loss: 1.0594 - acc: 0.6544 - val_loss: 1.1458 - val_acc: 0.6343\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 69s - loss: 1.0569 - acc: 0.6573 - val_loss: 1.1663 - val_acc: 0.6333\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 69s - loss: 1.0582 - acc: 0.6581 - val_loss: 1.1647 - val_acc: 0.6324\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 67s - loss: 1.0541 - acc: 0.6608 - val_loss: 1.1989 - val_acc: 0.6315\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 55s - loss: 1.0542 - acc: 0.6583 - val_loss: 1.1877 - val_acc: 0.6443\n",
      " 9888/10000 [============================>.] - ETA: 0s('Test score:', 1.1719327688217163)\n",
      "('Test accuracy:', 0.63590000000000002)\n"
     ]
    }
   ],
   "source": [
    "#configration_2(Refer configration file for details)\n",
    "model_shallow(*configration_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 62s - loss: 2.2911 - acc: 0.1246 - val_loss: 2.2488 - val_acc: 0.2248\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 62s - loss: 2.1386 - acc: 0.2093 - val_loss: 2.0214 - val_acc: 0.2715\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 75s - loss: 2.0277 - acc: 0.2561 - val_loss: 1.9785 - val_acc: 0.3041\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 77s - loss: 1.9698 - acc: 0.2826 - val_loss: 2.0098 - val_acc: 0.2850\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 88s - loss: 1.9167 - acc: 0.3069 - val_loss: 1.8502 - val_acc: 0.3487\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.8596 - acc: 0.3328 - val_loss: 1.7926 - val_acc: 0.3749\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.8045 - acc: 0.3554 - val_loss: 1.7545 - val_acc: 0.3813\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 88s - loss: 1.7492 - acc: 0.3701 - val_loss: 1.7108 - val_acc: 0.3892\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.6994 - acc: 0.3872 - val_loss: 1.6520 - val_acc: 0.4125\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.6546 - acc: 0.4025 - val_loss: 1.5961 - val_acc: 0.4320\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.6170 - acc: 0.4168 - val_loss: 1.5357 - val_acc: 0.4521\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 66s - loss: 1.5756 - acc: 0.4276 - val_loss: 1.5000 - val_acc: 0.4624\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.5441 - acc: 0.4430 - val_loss: 1.4437 - val_acc: 0.4830\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 80s - loss: 1.5127 - acc: 0.4527 - val_loss: 1.4277 - val_acc: 0.4907\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 61s - loss: 1.4600 - acc: 0.4726 - val_loss: 1.3693 - val_acc: 0.5093\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 61s - loss: 1.4298 - acc: 0.4819 - val_loss: 1.3425 - val_acc: 0.5209\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 61s - loss: 1.4026 - acc: 0.4937 - val_loss: 1.4197 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 61s - loss: 1.3821 - acc: 0.5048 - val_loss: 1.3027 - val_acc: 0.5377\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 60s - loss: 1.3593 - acc: 0.5122 - val_loss: 1.2760 - val_acc: 0.5457\n",
      " 9984/10000 [============================>.] - ETA: 0s('Test score:', 1.2633025310516357)\n",
      "('Test accuracy:', 0.55020000000000002)\n"
     ]
    }
   ],
   "source": [
    "#configration_3(Refer configration file for details)\n",
    "model_deep(*configration_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "40000/40000 [==============================] - 61s - loss: 1.7324 - acc: 0.3644 - val_loss: 1.3568 - val_acc: 0.5119\n",
      "Epoch 2/30\n",
      "40000/40000 [==============================] - 60s - loss: 1.3034 - acc: 0.5276 - val_loss: 1.1258 - val_acc: 0.5941\n",
      "Epoch 3/30\n",
      "40000/40000 [==============================] - 62s - loss: 1.1251 - acc: 0.6015 - val_loss: 0.9837 - val_acc: 0.6518\n",
      "Epoch 4/30\n",
      "40000/40000 [==============================] - 60s - loss: 1.0237 - acc: 0.6389 - val_loss: 0.9128 - val_acc: 0.6741\n",
      "Epoch 5/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.9596 - acc: 0.6600 - val_loss: 0.8341 - val_acc: 0.7071\n",
      "Epoch 6/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.9010 - acc: 0.6813 - val_loss: 0.8007 - val_acc: 0.7181\n",
      "Epoch 7/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.8505 - acc: 0.6999 - val_loss: 0.7711 - val_acc: 0.7332\n",
      "Epoch 8/30\n",
      "40000/40000 [==============================] - 61s - loss: 0.8166 - acc: 0.7141 - val_loss: 0.7642 - val_acc: 0.7360\n",
      "Epoch 9/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.7814 - acc: 0.7248 - val_loss: 0.7352 - val_acc: 0.7467\n",
      "Epoch 10/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.7488 - acc: 0.7359 - val_loss: 0.7142 - val_acc: 0.7500\n",
      "Epoch 11/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.7197 - acc: 0.7456 - val_loss: 0.7078 - val_acc: 0.7553\n",
      "Epoch 12/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.6938 - acc: 0.7558 - val_loss: 0.7333 - val_acc: 0.7484\n",
      "Epoch 13/30\n",
      "40000/40000 [==============================] - 61s - loss: 0.6730 - acc: 0.7620 - val_loss: 0.6708 - val_acc: 0.7680\n",
      "Epoch 14/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.6615 - acc: 0.7676 - val_loss: 0.6387 - val_acc: 0.7823\n",
      "Epoch 15/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.6401 - acc: 0.7733 - val_loss: 0.6592 - val_acc: 0.7700\n",
      "Epoch 16/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.6143 - acc: 0.7837 - val_loss: 0.6440 - val_acc: 0.7783\n",
      "Epoch 17/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.5965 - acc: 0.7899 - val_loss: 0.6290 - val_acc: 0.7869\n",
      "Epoch 18/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.5926 - acc: 0.7899 - val_loss: 0.6484 - val_acc: 0.7796\n",
      "Epoch 19/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.5742 - acc: 0.7973 - val_loss: 0.6243 - val_acc: 0.7863\n",
      "Epoch 20/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.5629 - acc: 0.7996 - val_loss: 0.6083 - val_acc: 0.7910\n",
      "Epoch 21/30\n",
      "40000/40000 [==============================] - 61s - loss: 0.5541 - acc: 0.8051 - val_loss: 0.6480 - val_acc: 0.7835\n",
      "Epoch 22/30\n",
      "40000/40000 [==============================] - 61s - loss: 0.5357 - acc: 0.8085 - val_loss: 0.6361 - val_acc: 0.7870\n",
      "Epoch 23/30\n",
      "40000/40000 [==============================] - 61s - loss: 0.5325 - acc: 0.8115 - val_loss: 0.6229 - val_acc: 0.7899\n",
      "Epoch 24/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.5130 - acc: 0.8168 - val_loss: 0.6138 - val_acc: 0.7922\n",
      "Epoch 25/30\n",
      "40000/40000 [==============================] - 61s - loss: 0.5087 - acc: 0.8198 - val_loss: 0.6387 - val_acc: 0.7896\n",
      "Epoch 26/30\n",
      "40000/40000 [==============================] - 61s - loss: 0.5045 - acc: 0.8208 - val_loss: 0.6319 - val_acc: 0.7898\n",
      "Epoch 27/30\n",
      "40000/40000 [==============================] - 61s - loss: 0.4944 - acc: 0.8224 - val_loss: 0.6045 - val_acc: 0.7955\n",
      "Epoch 28/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.4826 - acc: 0.8273 - val_loss: 0.5945 - val_acc: 0.7974\n",
      "Epoch 29/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.4733 - acc: 0.8308 - val_loss: 0.6052 - val_acc: 0.7980\n",
      "Epoch 30/30\n",
      "40000/40000 [==============================] - 60s - loss: 0.4633 - acc: 0.8337 - val_loss: 0.6010 - val_acc: 0.8020\n",
      "10000/10000 [==============================] - 3s     \n",
      "('Test score:', 0.62869126348495485)\n",
      "('Test accuracy:', 0.79359999999999997)\n"
     ]
    }
   ],
   "source": [
    "#configration_4(Refer configration file for details)\n",
    "model_deep(*configration_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1/Part 2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating 12000 random traing and testing data from orignal data\n",
    "x_total =np.concatenate((X_train,X_test),axis=0)\n",
    "y_total =np.concatenate((y_train,y_test),axis=0)\n",
    "seed = 6\n",
    "np.random.seed(seed)\n",
    "random = np.random.randint(0,60000,size=12000)\n",
    "x_train_new = x_total[random]\n",
    "y_train_new = y_total[random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating labels(code taken from the base code)\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n",
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('airplane', 1186)\n",
      "('automobile', 1261)\n",
      "('bird', 1215)\n",
      "('cat', 1201)\n",
      "('deer', 1213)\n",
      "('dog', 1177)\n",
      "('frog', 1217)\n",
      "('horse', 1184)\n",
      "('ship', 1184)\n",
      "('truck', 1162)\n"
     ]
    }
   ],
   "source": [
    "#summary of the images \n",
    "a= itemfreq(y_train_new)\n",
    "for i in range (0,len(a)):\n",
    "    print(labels['label_names'][i],a[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model is the model with config-4, using config 4 model for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "11936/12000 [============================>.] - ETA: 0s[3 9 8 ..., 9 6 0]\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open(\"configration_4_cifar10_architecture.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"configration_4_cifar10_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "predictions = loaded_model.predict_classes(x_train_new)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90275000000000005"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the prediction accuracy based on the model used\n",
    "accuracy_score(y_train_new, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 1172],\n",
       "       [   1, 1228],\n",
       "       [   2, 1089],\n",
       "       [   3, 1144],\n",
       "       [   4, 1215],\n",
       "       [   5, 1239],\n",
       "       [   6, 1286],\n",
       "       [   7, 1164],\n",
       "       [   8, 1249],\n",
       "       [   9, 1214]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the different class values in the predection\n",
    "class_values_prediction = itemfreq(predictions)\n",
    "class_values_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 32, 32, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to categorical\n",
    "y_train_new = np_utils.to_categorical(y_train_new, NB_CLASSES)\n",
    "\n",
    "\n",
    "# float and normalization\n",
    "x_train_new = x_train_new.astype('float32')\n",
    "x_train_new /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/20\n",
      "9600/9600 [==============================] - 15s - loss: 2.3030 - acc: 0.1024 - val_loss: 2.3028 - val_acc: 0.1050\n",
      "Epoch 2/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3028 - acc: 0.1024 - val_loss: 2.3027 - val_acc: 0.1050\n",
      "Epoch 3/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3026 - acc: 0.1090 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 4/20\n",
      "9600/9600 [==============================] - 15s - loss: 2.3026 - acc: 0.1044 - val_loss: 2.3031 - val_acc: 0.1050\n",
      "Epoch 5/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3026 - acc: 0.0993 - val_loss: 2.3031 - val_acc: 0.1050\n",
      "Epoch 6/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3026 - acc: 0.1033 - val_loss: 2.3028 - val_acc: 0.1050\n",
      "Epoch 7/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3025 - acc: 0.1022 - val_loss: 2.3028 - val_acc: 0.1050\n",
      "Epoch 8/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3024 - acc: 0.1049 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 9/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3026 - acc: 0.1048 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 10/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3024 - acc: 0.1020 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 11/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3024 - acc: 0.1058 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 12/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3025 - acc: 0.1030 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 13/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3026 - acc: 0.1039 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 14/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3024 - acc: 0.1067 - val_loss: 2.3030 - val_acc: 0.1050\n",
      "Epoch 15/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3024 - acc: 0.1047 - val_loss: 2.3030 - val_acc: 0.1050\n",
      "Epoch 16/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3026 - acc: 0.1024 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 17/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3026 - acc: 0.1047 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 18/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3024 - acc: 0.1044 - val_loss: 2.3029 - val_acc: 0.1050\n",
      "Epoch 19/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3024 - acc: 0.1054 - val_loss: 2.3030 - val_acc: 0.1050\n",
      "Epoch 20/20\n",
      "9600/9600 [==============================] - 14s - loss: 2.3025 - acc: 0.1055 - val_loss: 2.3030 - val_acc: 0.1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff79c3a7850>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation(ACT_FUNC))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation(ACT_FUNC))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation(ACT_FUNC))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(NEURONS))\n",
    "model.add(Activation(ACT_FUNC))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# train\n",
    "model.compile(loss=LOSS_FUNC, optimizer=OPTIM,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train_new, y_train_new, batch_size=BATCH_SIZE,\n",
    "          epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "          verbose=VERBOSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('configration_12000_cifar10_architecture.json', 'w').write(model_json)\n",
    "\n",
    "#And the weights learned by our deep network on the training set\n",
    "model.save_weights(\"configration_12000_cifar10_weights.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "11968/12000 [============================>.] - ETA: 0s[1 1 1 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('configration_12000_cifar10_architecture.json', \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model_1 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model_1.load_weights(\"configration_12000_cifar10_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "predictions_12000 = loaded_model_1.predict_classes(x_train_new)\n",
    "print(predictions_12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't handle mix of multilabel-indicator and binary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-16b834c4e7fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_12000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         raise ValueError(\"Can't handle mix of {0} and {1}\"\n\u001b[0;32m---> 82\u001b[0;31m                          \"\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't handle mix of multilabel-indicator and binary"
     ]
    }
   ],
   "source": [
    "#some error, not able to figure out\n",
    "accuracy_score(y_train_new, predictions_12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of config_4 is the best. The, tranning and validation accuracy in case of 12000 model is around 10 percent which is very poor. The reason behind it could be lack of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loaded_model_1 has the arcitecture and weights of 12000 image model. It is the deployment for the model and can be used to predict\n",
    "unseen data. loaded_model_1 has the arcitecture and weights of config 4 image model.\n",
    "\n",
    "There are no pros of using the 12000 image data model. confif 4 model is the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
